#!/usr/bin/env python3
"""
å¿«é€Ÿéƒ¨ç½²è„šæœ¬ - æ ¹æ®éœ€æ±‚è‡ªåŠ¨å®‰è£…ç›¸åº”ä¾èµ–å¹¶å¯åŠ¨æœåŠ¡
"""

import os
import sys
import subprocess
import argparse
import json
from pathlib import Path

def run_command(command, description=""):
    """æ‰§è¡Œå‘½ä»¤å¹¶å¤„ç†è¾“å‡º"""
    print(f"\n{'='*50}")
    print(f"æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {command}")
    print(f"{'='*50}")
    
    try:
        result = subprocess.run(command, shell=True, check=True, 
                              capture_output=True, text=True)
        print("âœ… æˆåŠŸ!")
        if result.stdout:
            print("è¾“å‡º:", result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        print("âŒ å¤±è´¥!")
        print("é”™è¯¯:", e.stderr)
        return False

def install_dependencies(mode="base"):
    """å®‰è£…ä¾èµ–åŒ…"""
    requirements_files = {
        "base": "requirements/base.txt",
        "inference": "requirements/inference.txt", 
        "training": "requirements/training.txt",
        "full": "requirements.txt"
    }
    
    if mode not in requirements_files:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
        return False
    
    req_file = requirements_files[mode]
    
    if not os.path.exists(req_file):
        print(f"âŒ ä¾èµ–æ–‡ä»¶ä¸å­˜åœ¨: {req_file}")
        return False
    
    print(f"\nğŸš€ å®‰è£… {mode} æ¨¡å¼ä¾èµ–...")
    
    # å‡çº§pip
    run_command("pip install --upgrade pip", "å‡çº§pip")
    
    # å®‰è£…ä¾èµ–
    if mode == "training":
        # è®­ç»ƒæ¨¡å¼éœ€è¦åˆ†æ‰¹å®‰è£…é¿å…å†²çª
        print("ğŸ“¦ è®­ç»ƒæ¨¡å¼ - åˆ†æ‰¹å®‰è£…ä¾èµ–...")
        
        # å…ˆå®‰è£…åŸºç¡€æ¡†æ¶
        basic_deps = [
            "torch==2.1.2+cu121",
            "torchvision==0.16.2+cu121", 
            "torchaudio==2.1.2+cu121",
            "accelerate==1.9.0"
        ]
        
        for dep in basic_deps:
            if not run_command(f"pip install {dep}", f"å®‰è£… {dep}"):
                return False
        
        # å†å®‰è£…å…¶ä»–ä¾èµ–
        run_command(f"pip install -r {req_file}", f"å®‰è£…è®­ç»ƒç¯å¢ƒä¾èµ–")
        
    else:
        run_command(f"pip install -r {req_file}", f"å®‰è£… {mode} æ¨¡å¼ä¾èµ–")
    
    return True

def check_gpu_support():
    """æ£€æŸ¥GPUæ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥GPUæ”¯æŒ...")
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… GPUå¯ç”¨: {gpu_name} (æ•°é‡: {gpu_count})")
            return True
        else:
            print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return False
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPU")
        return False

def start_service(service_type="gradio", port=7860):
    """å¯åŠ¨æœåŠ¡"""
    print(f"\nğŸš€ å¯åŠ¨ {service_type} æœåŠ¡ (ç«¯å£: {port})...")
    
    if service_type == "gradio":
        command = f"python demo_fastapi_gradio.py --port {port}"
    elif service_type == "fastapi":
        command = f"uvicorn demo_fastapi_gradio:app --host 0.0.0.0 --port {port}"
    elif service_type == "microservice":
        command = f"python speaker_recognition/core/service_registry.py"
    else:
        print(f"âŒ æœªçŸ¥æœåŠ¡ç±»å‹: {service_type}")
        return False
    
    print(f"å¯åŠ¨å‘½ä»¤: {command}")
    try:
        subprocess.run(command, shell=True)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æœåŠ¡å·²åœæ­¢")

def create_config_file(mode, gpu_available):
    """åˆ›å»ºé…ç½®æ–‡ä»¶"""
    config = {
        "deployment_mode": mode,
        "gpu_available": gpu_available,
        "services": {
            "speaker_recognition": True,
            "speech_synthesis": False,  # é»˜è®¤å…³é—­
            "voice_conversion": False,
            "noise_reduction": False,
            "audio_enhancement": False
        },
        "ports": {
            "gradio": 7860,
            "fastapi": 8000,
            "microservice_registry": 8001
        }
    }
    
    with open("deployment_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: deployment_config.json")

def main():
    parser = argparse.ArgumentParser(description="è¯­éŸ³è¯†åˆ«å¾®æœåŠ¡éƒ¨ç½²è„šæœ¬")
    
    parser.add_argument("--mode", choices=["base", "inference", "training", "full"],
                       default="base", help="éƒ¨ç½²æ¨¡å¼")
    
    parser.add_argument("--service", choices=["gradio", "fastapi", "microservice"],
                       default="gradio", help="å¯åŠ¨çš„æœåŠ¡ç±»å‹")
    
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡ç«¯å£")
    
    parser.add_argument("--skip-deps", action="store_true", 
                       help="è·³è¿‡ä¾èµ–å®‰è£…")
    
    parser.add_argument("--gpu-check", action="store_true",
                       help="ä»…æ£€æŸ¥GPUæ”¯æŒ")
    
    args = parser.parse_args()
    
    print("ğŸ¯ è¯­éŸ³è¯†åˆ«å¾®æœåŠ¡éƒ¨ç½²è„šæœ¬")
    print("=" * 50)
    
    if args.gpu_check:
        check_gpu_support()
        return
    
    # å®‰è£…ä¾èµ–
    if not args.skip_deps:
        if not install_dependencies(args.mode):
            print("âŒ ä¾èµ–å®‰è£…å¤±è´¥")
            return
    
    # æ£€æŸ¥GPUæ”¯æŒ
    gpu_available = check_gpu_support()
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_config_file(args.mode, gpu_available)
    
    # å¯åŠ¨æœåŠ¡
    print(f"\nğŸ‰ å‡†å¤‡å¯åŠ¨ {args.service} æœåŠ¡...")
    print("æç¤º: ä½¿ç”¨ Ctrl+C åœæ­¢æœåŠ¡")
    
    start_service(args.service, args.port)

if __name__ == "__main__":
    main()
